
cyber-dojo starter create S --git_url URL
  # url can be local git repo
cyber-dojo starter create S --git_urls_file FILENAME
  # default list lives inside base-image?
cyber-dojo up S
cyber-dojo ls

for each url
1. get branch names
     $ git ls-remote --heads URL
2. branch called start_point? if so...
     $ git clone --branch start_point --single-branch
     # this is assumed to be a LANGUAGES start-point url
3. otherwise...
     $ git clone URL
   if start_point/ and docker/ --> LANGUAGES
   if start_point/ only        --> CUSTOM
   else                        --> EXERCISES

For CFL repos that contain large docker/ dirs...
put the start_point/ into a branch called start_point
The intention is for this to happen only a few repos
and to fix them by doing proper installed, eg apk add package

ASIDE: Will be easy to silently copy the red_amber_green.rb files
from docker/ into start_point/ dir and thus make them available
for new ragger service.

clean. KEEP
down. KEEP
logs. KEEP
sh. KEEP
up. KEEP (modified)
update
>>>start-point DROP
>>>starter     ADD


#---------------------------------------------------------------------

.travis.yml
./pipe_build_test.sh
  >>>>>>
  >>>>>> check for docker/ start_point/ dir here? <<<<<<<<
    PROBLEM IS...current setup is dir based not URL based... See below ########
    otherwise...
    if [ ! -d ./docker ]; then
      git clone <url> --branch docker --single-branch ???docker???.???
    fi
    if [ ! -d ./start_point ]; then
      git clone <url> --branch start_point --single-branch ???docker???.???
    fi
  >>>>>>
${SH_DIR}/build_image_builder.sh
  docker build...
${SH_DIR}/test_image_builder.sh
  ${ROOT_DIR}/test/run.sh
  ${ROOT_DIR}/test/test_languages.sh
  ${ROOT_DIR}/test/test_testFrameworks.sh
    test_alpine()
      assertBuildImage /test/test-frameworks/alpine-java-junit
        build_image $1
          local src_dir=${ROOT_DIR}$1
          ${ROOT_DIR}/run_build_image.sh ${src_dir} > >(tee ${stdoutF}) 2> >(tee ${stderrF} >&2)

${ROOT_DIR}/run_build_image.sh ${src_dir}
  set_host_dir_permissions()
    sudo mkdir -p /cyber-dojo
    sudo chown -R 19663 /cyber-dojo
  volume_create
    create a data-volume-container holding src_dir
  network_create
  run $*
    docker run \
      --volume=/var/run/docker.sock:/var/run/docker.sock \
        cyberdojofoundation/image_builder \
          /app/outer_main.rb $*

    outer_main.rb
      'docker-compose',
        "--file #{my_dir}/docker-compose.yml",
        'run',
        'image_builder_inner',
          '/app/inner_main.rb',

    inner_main.rb
      src_dir = SourceDir.new(ENV['SRC_DIR'])
      src_dir.assert_create_start_point
         ###############################
         # runs commander's cyber-dojo start-point create ...
         #   create_cmd = "./#{script} start-point create #{name} --dir=#{dir_name}"
         #   assert_system create_cmd
         ###############################
      src_dir.check_all
         DockerDir.new(...).build_image(image_name)
           ImageBuilder.new(dir_name).build_image(image_FROM, name)
         StartPointDir.new(...).test_run
           assert_timed_run(:red)
           assert_timed_run(:amber)
           assert_timed_run(:green)





Adding the avatar users to an image.
This could be done on the base images (eg Alpine) rather than
on each of the test-framework images. This would obviously save
space (and time). How to tell if an image is a base image?
Maybe just have an option that is passed to the run script?
Maybe have an entry in the options.json file?
Currently the base image docker/ directories have a file
called image_name.json - could rename that.
But there are a lot of base images, one for each language.
Maybe instead rename image_name.json to options.json
only for base OS images.
Maybe better way is to maintain an up to date json data structure
of all the language repos and their dependencies. Then consult
this data structure to see whether an image is a base OS image.


# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
# TODO: Don't trigger_dependent_git_repos.
#       Instead have option to control when this run is for a single
#       repo or if for a chain starting with this repo.
#       If for a chain, start by creating a list of all the dependents.
#       Then build these in a chain, one by one.
#       This will considerably speed up Travis cycle-time since
#       Travis won't need to [docker pull] the image created on
#       the [docker push] step for the previous repo.
#       However, it may start to hit the 50min max time for
#       a single (public) Travis run.
#
# TODO: If TRAVIS env-var is defined check if DOCKER_ env-vars
#       are defined. If they are not issue error diagnostic and fail.
#       If TRAVIS env-var is not defined, check if DOCKER_ env-vars
#       are defined. If they are not issue warning diagnostic and continue
#       and do not do [docker login/push] commands.
#
#       If on Travis do a [git clone] of dependent repo
#       to continue the image-chain build.
#       If not on Travis, instead of doing [git clone]
#       assume the repo is in .. dir and simply use that.
#       However, that assumes the not just the current dir
#       but all its sibling dirs are available inside the
#       container. Currently I'm volume-mounting and then doing
#       a [cp -R] which is starting to sound impractical.
#       I think I need a data-container which I reuse.
#
#       Also, when not on Travis, instead of curling the
#       cyber-dojo script, check if cyber-dojo script is on path
#       and if so, use that. If no cyber-dojo script on path
#       then issue an error.
#       However that again confuses cyber-dojo being available
#       on the host which is not the same as the cyber-dojo
#       script being available inside the container.
#
# TODO: add information on how long red/amber/green runs take.
#       issue warning if they take too long?

# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

# TODO: could have a flag which, when given the name of a base repo,
#       and running locally, outputs a script of dependents, in order,
#       which can be piped into bash/sh.
#       Eg
#       image_builder/run_build_image ...../python-3.6.1
#       image_builder/run_build_image ...../python-pytest
#       image_builder/run_build_image ...../python-behave
#       image_builder/run_build_image ...../python-unittest


# TODO: are we are on Travis or running locally?
# If on Travis, then only the current dir needs to be
# volume mounted so it is visible inside the container.
# Dependent repos will be git clones.
# If locally, then I need to volume-mount the base-dir
# so that all dependent repos will be available inside
# the container without doing a git clone. I could of course
# still do git clones, but I want to be able to edit
# multiple repos in the chain locally.

# TODO: volume-mount the cyber-dojo script inside the
# container if running locally? (I run it to check a
# dir's start_point/ is valid).
# This would avoid needing to curl it.

# TODO: think I need to volume-mount the json file
# containing the dependency triples. Unless this is
# parameterizable the script won't be much use for
# someone testing out their own language/test repos.
# Or maybe make using this dependency list optional???
# Default could be just to build for the dir named on
# the command-line. How to extend beyond that?
# If a json file is named then use that?
# That would work well on Travis.
# Could also use the base-dir of the dir given.
# That would work well for a local build.


# display timings
# - - - - - - - -
# I would like to scrape the Travis logs for all the language
# repos and display the red/amber/green test durations in a
# sorted list.


# display version numbers
# - - - - - - - - - - - -
# the base-language versions be gathered from the dependency table
# I can split the table into named sections and create a section for the
# base-language entries and then the version numbers
# can be harvested from that.

# TODO: add checks that the version numbers of
# the repo-name and the image-name match up.

# TODO: add check that the repo-names version
# numbers use -1.2.3 format.

# TODO: add check that the image-names for
# base images use version numbers use :1.2.3 format.

# TODO: add check that the image-names for
# language+test images do NOT use version numbers.
# Is this what I want?
# I could a version number here and then _also_
# tag the image as latest (and push that)
# (possibly with a command line option).
# Orthogonal to that, what version number do I use?
# The language's or the test framework's? Or both?
